#!/usr/bin/env python3
# ---------------------------------------------------------------
# Â© 2025 Mobile Perception Systems Lab at TU/e. All rights reserved.
# Licensed under the MIT License.
# ---------------------------------------------------------------

"""
build_vis_cache.py - æ„å»ºè§†è§‰åµŒå…¥ç¼“å­˜ï¼ˆPyTorch Lightningç‰ˆæœ¬ï¼‰

ä¸ºCOCO Panopticæ•°æ®é›†ä¸­çš„æ¯ä¸ªå›¾åƒç”ŸæˆSigLIPè§†è§‰åµŒå…¥ï¼Œè¾“å‡ºï¼š
- cache/vis_embed/<imageID>.npz                  (æ¯ä¸ªå›¾åƒçš„æ‰€æœ‰segment embeddings)
- cache/vis_embed/image_index.tsv                (å›¾åƒç´¢å¼•æ–‡ä»¶)

æ¯ä¸ªNPZæ–‡ä»¶åŒ…å«ï¼š
- 'segment_ids': è¯¥å›¾åƒçš„æ‰€æœ‰segment IDæ•°ç»„
- 'embeddings': å¯¹åº”çš„512ç»´åµŒå…¥å‘é‡æ•°ç»„ (fp16)
- 'labels': å¯¹åº”çš„ç±»åˆ«æ ‡ç­¾æ•°ç»„

TSVç´¢å¼•æ–‡ä»¶æ ¼å¼:
    <image_id> <tab> <feat_file> <tab> <num_segments>
"""

import os
import sys
import json
import argparse
import tqdm
import numpy as np
from pathlib import Path
from PIL import Image
from typing import List, Tuple, Dict, Any, Optional

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Subset, SequentialSampler

import torchvision.transforms.v2.functional as F
import lightning
from lightning.pytorch import Trainer
from lightning.pytorch.callbacks import BasePredictionWriter

from eomt.novic.embedders import Embedder
from eomt.datasets.coco_panoptic_directory import COCOPanopticDirectory
from eomt.datasets.lightning_data_module import LightningDataModule


class VisualEmbeddingInferenceModule(lightning.LightningModule):
    """ç”¨äºæ¨ç†çš„Lightningæ¨¡å—"""
    
    def __init__(
        self,
        embedder_spec: str = "openclip:timm/ViT-B-16-SigLIP",
        bg_alpha: float = 0.3,
        pad_ratio: float = 0.1,
        cache_dir: str = "/tmp/vis_embed",
    ):
        super().__init__()
        self.embedder_spec = embedder_spec
        self.bg_alpha = bg_alpha
        self.pad_ratio = pad_ratio
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.total_images_processed = 0
        self.total_segments_processed = 0
        self.total_images_skipped = 0
        
    def setup(self, stage: str):
        """è®¾ç½®åµŒå…¥å™¨"""
        if stage == "predict":
            print(f"[Rank {self.global_rank}] åŠ è½½SigLIPæ¨¡å‹: {self.embedder_spec}")
            self.embedder = Embedder.create(
                spec=self.embedder_spec,
                amp=True,
                device=self.device,
                inference_batch_size=64,
                image_batch_size=32,
                load_model=True,
                compile_model=False,
            )
            self.image_transform = self.embedder.get_image_transform()
    
    def predict_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -> Dict[str, Any]:
        """é¢„æµ‹æ­¥éª¤ï¼šå¤„ç†ä¸€ä¸ªbatchçš„æ•°æ®"""
        batch_results = []
        
        # æå–æ‰¹æ¬¡ä¿¡æ¯
        batch_info = self.extract_image_and_segment_ids(batch)
        
        for image_id, image_tensor, masks, labels, target_ids in batch_info:
            if not masks or not target_ids:
                continue
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
            feat_file = f"{image_id}.npz"
            out_path = self.cache_dir / feat_file
            
            if out_path.exists():
                self.total_images_skipped += 1
                # è¯»å–å·²å­˜åœ¨æ–‡ä»¶ä»¥è·å–segmentæ•°é‡ç”¨äºç´¢å¼•
                try:
                    existing_data = np.load(out_path)
                    num_segments = len(existing_data['segment_ids'])
                    batch_results.append({
                        'image_id': image_id,
                        'feat_file': feat_file,
                        'num_segments': num_segments,
                        'status': 'skipped'
                    })
                except:
                    pass
                continue
            
            # ä¸ºæ‰€æœ‰maskå‡†å¤‡å›¾åƒ
            processed_images, valid_indices, valid_segment_ids, valid_labels = self.prep_mask_image(
                image_tensor, masks, labels, target_ids
            )
            
            if not processed_images:
                continue
            
            # æ‰¹é‡å¤„ç†å›¾åƒ
            image_tensors = []
            for pil_img in processed_images:
                tensor = self.image_transform(pil_img)
                image_tensors.append(tensor)
            
            if image_tensors:
                # æ‰¹é‡æ¨ç†
                batch_tensor = torch.stack(image_tensors)
                
                with self.embedder.inference_mode():
                    embeddings = self.embedder.inference_image(batch_tensor)
                
                # ä¿å­˜è¯¥å›¾åƒçš„æ‰€æœ‰embeddings
                if self.save_image_embeddings(
                    image_id, valid_segment_ids, embeddings, valid_labels
                ):
                    self.total_images_processed += 1
                    self.total_segments_processed += len(embeddings)
                    
                    batch_results.append({
                        'image_id': image_id,
                        'feat_file': feat_file,
                        'num_segments': len(embeddings),
                        'status': 'processed'
                    })
        
        return {
            'batch_idx': batch_idx,
            'results': batch_results,
            'rank': self.global_rank
        }
    
    def extract_image_and_segment_ids(self, batch):
        """ä»æ•°æ®åŠ è½½å™¨æ‰¹æ¬¡ä¸­æå–å›¾åƒIDå’Œsegment IDä¿¡æ¯"""
        images, targets = batch
        batch_info = []
        
        for i in range(len(images)):
            image = images[i]
            target = targets[i]
            
            # ç›´æ¥ä»targetä¸­è·å–å›¾åƒID
            image_id = target.get('image_id', f'image_{i}')
            
            # DirectoryDatasetè¿”å›çš„targetæ˜¯ä¸€ä¸ªå­—å…¸
            if isinstance(target, dict):
                # ä»å­—å…¸ä¸­æå–æ•°æ®
                masks_tensor = target.get('masks', torch.empty(0, 0, 0))
                labels_tensor = target.get('labels', torch.empty(0))
                target_ids_tensor = target.get('target_ids', torch.empty(0))
                
                # è½¬æ¢ä¸ºåˆ—è¡¨å½¢å¼
                masks = []
                if isinstance(masks_tensor, torch.Tensor) and masks_tensor.numel() > 0:
                    if masks_tensor.dim() >= 3:
                        # masks shape: [N, H, W]
                        masks = [masks_tensor[j] for j in range(masks_tensor.shape[0])]
                    else:
                        # å¦‚æœåªæœ‰ä¸€ä¸ªmask
                        masks = [masks_tensor]
                
                labels = []
                if isinstance(labels_tensor, torch.Tensor) and labels_tensor.numel() > 0:
                    labels = labels_tensor.tolist()
                
                target_ids = []
                if isinstance(target_ids_tensor, torch.Tensor) and target_ids_tensor.numel() > 0:
                    target_ids = target_ids_tensor.tolist()
                    # ç¡®ä¿target_idsæ˜¯æ•´æ•°åˆ—è¡¨
                    target_ids = [int(tid) if isinstance(tid, (int, float)) else tid for tid in target_ids]
                
            else:
                print(f"[Rank {self.global_rank}] æ— æ³•è§£ætargetæ ¼å¼: {type(target)}")
                continue
            
            # éªŒè¯æ•°æ®çš„æœ‰æ•ˆæ€§
            if not masks or not target_ids:
                continue
                
            if len(masks) != len(target_ids):
                # å–è¾ƒå°çš„é•¿åº¦
                min_len = min(len(masks), len(target_ids))
                masks = masks[:min_len]
                target_ids = target_ids[:min_len]
                if labels:
                    labels = labels[:min_len]
                
            batch_info.append((image_id, image, masks, labels, target_ids))
        
        return batch_info
    
    def prep_mask_image(self, image: torch.Tensor, masks: list, labels: list, target_ids: list):
        """ä¸ºæ¯ä¸ªmaskå‡†å¤‡è£å‰ªå’Œæ··åˆåçš„å›¾åƒ"""
        if not masks:
            return [], [], [], []
        
        # å°†å›¾åƒè½¬æ¢ä¸ºnumpyæ•°ç»„ [H, W, C]ï¼ŒèŒƒå›´ [0, 1]
        if image.dim() == 3:
            rgb = image.permute(1, 2, 0).cpu().numpy()
        else:
            rgb = image.cpu().numpy()
        
        # ç¡®ä¿æ˜¯RGBæ ¼å¼ä¸”åœ¨[0,1]èŒƒå›´å†…
        if rgb.max() > 1.0:
            rgb = rgb / 255.0
        
        processed_images = []
        valid_indices = []
        valid_segment_ids = []
        valid_labels = []
        
        for i, mask in enumerate(masks):
            try:
                # è½¬æ¢maskä¸ºnumpyæ•°ç»„
                mask_np = mask.cpu().numpy().astype(bool)
                
                # æ£€æŸ¥maskæ˜¯å¦æœ‰æ•ˆ
                if not mask_np.any():
                    continue
                    
                # åº”ç”¨èƒŒæ™¯æ··åˆ
                alpha = np.where(mask_np, 1.0, self.bg_alpha).astype(np.float32)
                blended = rgb * alpha[..., None] + (1 - alpha[..., None]) * 0.5

                # è·å–maskçš„è¾¹ç•Œæ¡†
                ys, xs = np.where(mask_np)
                if len(ys) == 0 or len(xs) == 0:
                    continue
                    
                y0, y1 = ys.min(), ys.max()
                x0, x1 = xs.min(), xs.max()
                
                # æ·»åŠ å¡«å……
                pad_y = int(self.pad_ratio * (y1 - y0 + 1))
                pad_x = int(self.pad_ratio * (x1 - x0 + 1))
                y0 = max(0, y0 - pad_y)
                y1 = min(mask_np.shape[0], y1 + pad_y + 1)
                x0 = max(0, x0 - pad_x)
                x1 = min(mask_np.shape[1], x1 + pad_x + 1)
                
                # è£å‰ªå›¾åƒ
                crop = blended[y0:y1, x0:x1]

                # è½¬æ¢ä¸ºPILå›¾åƒ
                crop_pil = Image.fromarray((crop * 255).astype(np.uint8))
                processed_images.append(crop_pil)
                valid_indices.append(i)
                
                # ä¿å­˜å¯¹åº”çš„segment_idå’Œlabel
                if i < len(target_ids):
                    valid_segment_ids.append(target_ids[i])
                else:
                    valid_segment_ids.append(0)  # é»˜è®¤å€¼
                    
                if i < len(labels):
                    valid_labels.append(labels[i])
                else:
                    valid_labels.append(0)  # é»˜è®¤å€¼
                
            except Exception as e:
                print(f"[Rank {self.global_rank}] å¤„ç†mask {i}æ—¶å‡ºé”™: {e}")
                continue

        return processed_images, valid_indices, valid_segment_ids, valid_labels
    
    def save_image_embeddings(self, image_id: str, segment_ids: list, 
                             embeddings, labels: list) -> bool:
        """å°†ä¸€ä¸ªå›¾åƒçš„æ‰€æœ‰segment embeddingsä¿å­˜åˆ°NPZæ–‡ä»¶"""
        # æ£€æŸ¥è¾“å…¥æœ‰æ•ˆæ€§
        if not segment_ids:
            return False
        
        # å¤„ç†embeddingsçš„ä¸åŒç±»å‹
        if isinstance(embeddings, torch.Tensor):
            if embeddings.numel() == 0:  # ç©ºå¼ é‡
                return False
            embeddings_array = embeddings.cpu().numpy().astype(np.float16)
        elif hasattr(embeddings, '__len__'):  # åˆ—è¡¨æˆ–ç±»ä¼¼åºåˆ—
            if len(embeddings) == 0:
                return False
            embeddings_array = np.stack([emb.cpu().numpy().astype(np.float16) for emb in embeddings])
        else:
            return False
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        segment_ids_array = np.array(segment_ids, dtype=np.int32)
        labels_array = np.array(labels, dtype=np.int32) if labels else np.zeros(len(segment_ids), dtype=np.int32)
        
        # ä¿å­˜æ–‡ä»¶
        feat_file = f"{image_id}.npz"
        out_path = self.cache_dir / feat_file
        
        np.savez_compressed(
            out_path,
            segment_ids=segment_ids_array,
            embeddings=embeddings_array,
            labels=labels_array
        )
        
        return True


class EmbeddingResultWriter(BasePredictionWriter):
    """å¤„ç†é¢„æµ‹ç»“æœçš„å†™å…¥å™¨"""
    
    def __init__(self, cache_dir: str, write_interval: str = "batch"):
        super().__init__(write_interval)
        self.cache_dir = Path(cache_dir)
        self.index_file = self.cache_dir / "image_index.tsv"
        self.results_buffer = []
        
        # åœ¨ä¸»è¿›ç¨‹åˆå§‹åŒ–ç´¢å¼•æ–‡ä»¶
        if write_interval == "batch":
            self._init_index_file()
    
    def _init_index_file(self):
        """åˆå§‹åŒ–ç´¢å¼•æ–‡ä»¶"""
        with open(self.index_file, "w", encoding='utf-8') as f:
            f.write("image_id\tfeat_file\tnum_segments\n")
    
    def write_on_batch_end(
        self, 
        trainer: Trainer, 
        pl_module: lightning.LightningModule, 
        prediction: Any, 
        batch_indices: List[int], 
        batch: Any, 
        batch_idx: int, 
        dataloader_idx: int
    ):
        """åœ¨æ¯ä¸ªbatchç»“æŸæ—¶å†™å…¥ç»“æœ"""
        if prediction and 'results' in prediction:
            self.results_buffer.extend(prediction['results'])
            
            # æ¯å¤„ç†100ä¸ªbatchæˆ–åœ¨æœ€åå†™å…¥ç´¢å¼•æ–‡ä»¶
            if len(self.results_buffer) >= 100 or batch_idx % 100 == 0:
                self._write_index_batch()
    
    def write_on_epoch_end(
        self, 
        trainer: Trainer, 
        pl_module: lightning.LightningModule, 
        predictions: List[Any], 
        batch_indices: List[List[int]]
    ):
        """åœ¨epochç»“æŸæ—¶å†™å…¥å‰©ä½™ç»“æœ"""
        self._write_index_batch()
    
    def _write_index_batch(self):
        """æ‰¹é‡å†™å…¥ç´¢å¼•"""
        if not self.results_buffer:
            return
            
        with open(self.index_file, "a", encoding='utf-8') as f:
            for result in self.results_buffer:
                if result['status'] in ['processed', 'skipped']:
                    f.write(f"{result['image_id']}\t{result['feat_file']}\t{result['num_segments']}\n")
        
        self.results_buffer.clear()


class VisCacheDataModule(LightningDataModule):
    """ç”¨äºæ„å»ºè§†è§‰ç¼“å­˜çš„æ•°æ®æ¨¡å—"""
    
    def __init__(
        self,
        data_path: str,
        batch_size: int = 8,
        num_workers: int = 4,
        shuffle: bool = False,
        **kwargs
    ):
        super().__init__(
            path=data_path,
            batch_size=batch_size,
            num_workers=num_workers,
            img_size=(640, 640),
            num_classes=133,  # COCO Panoptic classes
            check_empty_targets=False,
            **kwargs
        )
        
        # COCO Panopticçš„stuffç±»åˆ« (80-132)
        self.stuff_classes = list(range(80, 133))
        self.shuffle = shuffle
    
    def setup(self, stage: str):
        """è®¾ç½®æ•°æ®é›†"""
        if stage == "predict":
            self.predict_dataset = COCOPanopticDirectory(
                path=self.path,
                stuff_classes=self.stuff_classes,
                batch_size=self.dataloader_kwargs["batch_size"],
                num_workers=self.dataloader_kwargs["num_workers"],
                img_size=self.img_size,
                check_empty_targets=self.check_empty_targets,
                color_jitter_enabled=False,
                scale_range=(1.0, 1.0),  # ä¸è¿›è¡Œç¼©æ”¾å˜æ¢ï¼Œä¿æŒåŸå§‹å°ºå¯¸
            )
            self.predict_dataset.setup("fit")
    
    def predict_dataloader(self):
        """è¿”å›é¢„æµ‹æ•°æ®åŠ è½½å™¨ï¼ˆå…³é—­shuffleï¼‰"""
        return DataLoader(
            self.predict_dataset.train_dataset,
            batch_size=self.dataloader_kwargs["batch_size"],
            shuffle=self.shuffle,
            num_workers=self.dataloader_kwargs["num_workers"],
            pin_memory=self.dataloader_kwargs.get("pin_memory", True),
            persistent_workers=self.dataloader_kwargs.get("persistent_workers", True),
            collate_fn=self.train_collate,
            drop_last=False,
        )

'''
python -m eomt.datasets.build_vis_cache \
  --data_path /home/host_ssd/coconut_dataset/coco \
  --cache_dir /home/host_ssd/coconut_dataset/vis_embed \
  --batch_size 32 \
  --num_workers 32 \
  --devices 8 \
  --strategy ddp
'''

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    ap = argparse.ArgumentParser(description="æ„å»ºCOCO Panopticè§†è§‰åµŒå…¥ç¼“å­˜ï¼ˆLightningç‰ˆæœ¬ï¼‰")
    ap.add_argument("--data_path", 
                    default="/home/host_ssd/coconut_dataset/coco",
                    help="COCOæ•°æ®é›†æ ¹ç›®å½•è·¯å¾„")
    ap.add_argument("--cache_dir", 
                    default="/home/host_ssd/coconut_dataset/vis_embed",
                    help="ç¼“å­˜è¾“å‡ºç›®å½•")
    ap.add_argument("--embedder_spec", 
                    default="openclip:timm/ViT-B-16-SigLIP",
                    help="SigLIPæ¨¡å‹è§„æ ¼")
    ap.add_argument("--batch_size", 
                    type=int, 
                    default=8,
                    help="æ•°æ®åŠ è½½å™¨æ‰¹å¤§å°")
    ap.add_argument("--num_workers", 
                    type=int, 
                    default=8,
                    help="æ•°æ®åŠ è½½å™¨å·¥ä½œè¿›ç¨‹æ•°")
    ap.add_argument("--devices", 
                    type=int, 
                    default=1,
                    help="ä½¿ç”¨çš„GPUæ•°é‡")
    ap.add_argument("--accelerator", 
                    default="gpu",
                    help="åŠ é€Ÿå™¨ç±»å‹")
    ap.add_argument("--strategy", 
                    default="auto",
                    help="åˆ†å¸ƒå¼ç­–ç•¥")
    ap.add_argument("--bg_alpha", 
                    type=float, 
                    default=0.3,
                    help="èƒŒæ™¯æ··åˆé€æ˜åº¦")
    ap.add_argument("--pad_ratio", 
                    type=float, 
                    default=0.1,
                    help="è£å‰ªæ—¶çš„å¡«å……æ¯”ä¾‹")
    ap.add_argument("--shuffle", 
                    action="store_true",
                    help="æ˜¯å¦å¯¹æ•°æ®è¿›è¡Œshuffleï¼ˆé»˜è®¤å…³é—­ä»¥ä¿è¯ç¡®å®šæ€§é¡ºåºï¼‰")
    return ap.parse_args()


def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()
    
    # è®¾ç½®è·¯å¾„
    cache_dir = Path(args.cache_dir).resolve()
    cache_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"ç¼“å­˜ç›®å½•: {cache_dir}")
    print(f"æ•°æ®è·¯å¾„: {args.data_path}")
    print(f"ä½¿ç”¨è®¾å¤‡: {args.devices} x {args.accelerator}")
    print(f"SigLIPæ¨¡å‹: {args.embedder_spec}")
    
    # åˆ›å»ºLightningæ¨¡å—
    model = VisualEmbeddingInferenceModule(
        embedder_spec=args.embedder_spec,
        bg_alpha=args.bg_alpha,
        pad_ratio=args.pad_ratio,
        cache_dir=str(cache_dir),
    )
    
    # åˆ›å»ºæ•°æ®æ¨¡å—
    data_module = VisCacheDataModule(
        data_path=args.data_path,
        batch_size=args.batch_size,
        num_workers=args.num_workers,
        shuffle=args.shuffle,
    )
    
    # åˆ›å»ºç»“æœå†™å…¥å™¨
    writer = EmbeddingResultWriter(cache_dir=str(cache_dir))
    
    # åˆ›å»ºTrainer
    trainer = Trainer(
        accelerator=args.accelerator,
        devices=args.devices,
        strategy=args.strategy,
        precision="16-mixed",
        callbacks=[writer],
        enable_model_summary=False,
        enable_progress_bar=True,
        logger=False,  # ç¦ç”¨æ—¥å¿—è®°å½•å™¨
    )
    
    # å¼€å§‹æ¨ç†
    print("ğŸš€ å¼€å§‹æ„å»ºè§†è§‰åµŒå…¥ç¼“å­˜...")
    trainer.predict(model, datamodule=data_module)
    
    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
    print(f"\nâœ… å®Œæˆï¼")
    print(f"ç¼“å­˜ç›®å½•: {cache_dir}")
    print(f"ç´¢å¼•æ–‡ä»¶: {cache_dir / 'image_index.tsv'}")
    
    # è¾“å‡ºä½¿ç”¨ç¤ºä¾‹
    print(f"\nğŸ’¡ ä½¿ç”¨ç¤ºä¾‹:")
    print(f"```python")
    print(f"import numpy as np")
    print(f"")
    print(f"# åŠ è½½æŸä¸ªå›¾åƒçš„embeddings")
    print(f"data = np.load('{cache_dir}/{{image_id}}.npz')")
    print(f"segment_ids = data['segment_ids']      # shape: (N,)")
    print(f"embeddings = data['embeddings']        # shape: (N, 512)")
    print(f"labels = data['labels']                # shape: (N,)")
    print(f"```")


if __name__ == "__main__":
    main()
